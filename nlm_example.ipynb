{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlm_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEk5mjUYHlr8KKnTQNx3vw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipavlopoulos/lm/blob/master/nlm_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNe41W0qzuuY"
      },
      "source": [
        "# Neural Language Model example\n",
        "This is an example of how to use a GRU RNN model to predict the next most probable word, given an excerpt from the Bible. You could also change the code to use LSTMs or a different corpus. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0-SbOuQxtmn"
      },
      "source": [
        "### Installations\n",
        "* The LM package.\n",
        "* The Natural Language Toolkit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3Bm9wnW1BKN",
        "outputId": "c75d3492-3501-4f07-ca76-41d8d358e613",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! git clone https://github.com/ipavlopoulos/lm.git\n",
        "! pip install nltk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'lm' already exists and is not an empty directory.\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GeVvYgVx14w"
      },
      "source": [
        "### Download some textÂ for training\n",
        "* Download Gutenberg.\n",
        "* Load the Bible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlP7HI2HfJja",
        "outputId": "a98300b3-b9cf-47c9-9fcc-4ccf82ba04d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk;nltk.download('gutenberg')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZaNlwyNfOs0",
        "outputId": "95c68353-1208-4823-b5ef-7024c78ae92c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text = nltk.corpus.gutenberg.raw('bible-kjv.txt')\n",
        "print(text[500:1000])\n",
        "print(len(text), \"characters\")\n",
        "print(len(text.split()), \"tokens\")\n",
        "print(len(set(text.split())), \"word types\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y, and the darkness he called Night.\n",
            "And the evening and the morning were the first day.\n",
            "\n",
            "1:6 And God said, Let there be a firmament in the midst of the waters,\n",
            "and let it divide the waters from the waters.\n",
            "\n",
            "1:7 And God made the firmament, and divided the waters which were\n",
            "under the firmament from the waters which were above the firmament:\n",
            "and it was so.\n",
            "\n",
            "1:8 And God called the firmament Heaven. And the evening and the\n",
            "morning were the second day.\n",
            "\n",
            "1:9 And God said, Let the waters under the heav\n",
            "4332554 characters\n",
            "821133 tokens\n",
            "33461 word types\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnv1clMFyK-S"
      },
      "source": [
        "### Train your GRU model\n",
        "* Use many epochs with early stopping at one patience for speed.\n",
        "* Limit training to 100K steps, to avoid memory issues.\n",
        "* Limit to a 20K words vocabulary, but if the out-of-vocabulary (oov) token appears frequently as a suggestion, you may want to increase this. For the Bible, for example, with the current setting, we mask 13,461 infrequent word types with the `[oov]` pseudo token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofjebtQoetiJ",
        "outputId": "26143f15-194a-464e-d24e-a10a1d38d396",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from lm.neural.models import RNN\n",
        "gru = RNN(epochs=100, \n",
        "          vocab_size=20000, \n",
        "          use_gru=True, \n",
        "          patience=1, \n",
        "          max_steps=100000, \n",
        "          batch_size=32)\n",
        "gru.train(text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 20000\n",
            "Total Sequences: 99997\n",
            "Epoch 1/100\n",
            "2813/2813 [==============================] - 120s 43ms/step - loss: 5.8679 - accuracy: 0.1539 - val_loss: 5.6611 - val_accuracy: 0.2258\n",
            "Epoch 2/100\n",
            "2813/2813 [==============================] - 120s 43ms/step - loss: 4.8990 - accuracy: 0.2230 - val_loss: 5.5713 - val_accuracy: 0.2437\n",
            "Epoch 3/100\n",
            "2813/2813 [==============================] - 116s 41ms/step - loss: 4.4265 - accuracy: 0.2525 - val_loss: 5.6210 - val_accuracy: 0.2520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tILEErt7y5YO"
      },
      "source": [
        "* Now, let's see some suggestions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QsDZ5SPhseA",
        "outputId": "3171d6dd-7d93-4811-e696-db28c423b2cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "context = \"from the waters\"\n",
        "gru.generate_next_gram(context, top_n=3)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['of', 'shall', 'oov']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aahrJ8h5zQuA"
      },
      "source": [
        "* Note that you might want to exclude `[oov]` from the results, since it is not very informative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHa3BTs_kM1S",
        "outputId": "8a5542a2-be7b-4061-d4d1-ec7c651610d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "suggested_words = gru.generate_next_gram(context, top_n=5)\n",
        "suggested_words = [word for word in suggested_words if word != \"oov\"]\n",
        "suggested_words[:3]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['of', 'shall', 'is']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}